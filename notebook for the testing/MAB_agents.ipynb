{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607b5c905d06e29b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:47:53.115277Z",
     "start_time": "2025-04-24T14:47:53.110684Z"
    }
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "    def __init__(self, n_arms, epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)\n",
    "        return np.argmax(self.values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] += (reward - self.values[arm]) / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5a1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097eff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "class GPTAgent:\n",
    "    def __init__(self, n_arms, model=\"gpt-4\"):\n",
    "        self.n_arms = n_arms\n",
    "        self.model = model\n",
    "        self.client = openai.OpenAI()  # Requires openai>=1.0.0 and OPENAI_API_KEY env var\n",
    "        self.history = []\n",
    "\n",
    "    def _build_prompt(self):\n",
    "        prompt = (\n",
    "            \"You are an agent playing a multi-armed bandit game with {n} arms.\\n\"\n",
    "            \"At each step, you must choose an arm (0 to {max_idx}) to maximize your total reward.\\n\"\n",
    "            \"You will be given the history of (arm, reward) pairs so far.\\n\"\n",
    "            \"Return ONLY the next arm index to pull, in the format:\\n\"\n",
    "            \"Next Arm Index: <index>\\n\"\n",
    "            \"Example: Next Arm Index: 2\\n\"\n",
    "            \"History:\\n\"\n",
    "        ).format(n=self.n_arms, max_idx=self.n_arms-1)\n",
    "        for arm, reward in self.history:\n",
    "            prompt += f\"Arm: {arm}, Reward: {reward}\\n\"\n",
    "        prompt += \"What is your next arm index?\"\n",
    "        return prompt\n",
    "\n",
    "    def _parse_response(self, response):\n",
    "        match = re.search(r\"Next Arm Index: (\\\\d+)\", response)\n",
    "        if match:\n",
    "            idx = int(match.group(1))\n",
    "            if 0 <= idx < self.n_arms:\n",
    "                return idx\n",
    "        # fallback: try to find any integer in the response\n",
    "        match = re.search(r\"(\\\\d+)\", response)\n",
    "        if match:\n",
    "            idx = int(match.group(1))\n",
    "            if 0 <= idx < self.n_arms:\n",
    "                return idx\n",
    "        # fallback: random\n",
    "        return random.randint(0, self.n_arms-1)\n",
    "\n",
    "    def choose_arm(self):\n",
    "        prompt = self._build_prompt()\n",
    "        retries = 3\n",
    "        delay = 2\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=20,\n",
    "                    temperature=0.0,\n",
    "                )\n",
    "                text = response.choices[0].message.content.strip()\n",
    "                idx = self._parse_response(text)\n",
    "                return idx\n",
    "            except Exception as e:\n",
    "                print(f\"OpenAI API error: {e}. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "        # fallback: random\n",
    "        return random.randint(0, self.n_arms-1)\n",
    "\n",
    "    def select_arm(self):\n",
    "        return self.choose_arm()\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.history.append((arm, reward))\n",
    "\n",
    "    def reset(self):\n",
    "        self.history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baef76a8e06c9b50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:47:53.655146Z",
     "start_time": "2025-04-24T14:47:53.649424Z"
    }
   },
   "outputs": [],
   "source": [
    "class UCBAgent:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        self.total_count = 0\n",
    "\n",
    "    def select_arm(self):\n",
    "        self.total_count += 1\n",
    "        for i in range(self.n_arms):\n",
    "            if self.counts[i] == 0:\n",
    "                return i  # explore untried arms\n",
    "        ucb_values = self.values + np.sqrt(2 * np.log(self.total_count) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] += (reward - self.values[arm]) / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a714d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingAgent():\n",
    "    \"\"\"Thompson Sampling bandit agent for Bernoulli rewards.\"\"\"\n",
    "    def __init__(self, num_arms, name=\"Thompson Sampling\"):\n",
    "        super().__init__(num_arms, name=name)\n",
    "        self.reset()\n",
    "\n",
    "    def choose_arm(self, current_round=None):\n",
    "        # Sample from the Beta posterior distribution for each arm\n",
    "        sampled_theta = [stats.beta.rvs(a, b) for a, b in zip(self.alpha, self.beta)]\n",
    "\n",
    "        # Choose the arm with the highest sampled value\n",
    "        best_arms = np.where(sampled_theta == np.max(sampled_theta))[0]\n",
    "        return np.random.choice(best_arms)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        super().update(arm, reward) # Updates counts\n",
    "        # Update Beta distribution parameters\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.alpha = np.ones(self.num_arms, dtype=float) # Successes + 1\n",
    "        self.beta = np.ones(self.num_arms, dtype=float)  # Failures + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988805d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c29197511217761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:47:54.081410Z",
     "start_time": "2025-04-24T14:47:54.077050Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ThompsonSamplingAgent.__init__() got an unexpected keyword argument 'n_arms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 67\u001b[0m\n\u001b[1;32m     61\u001b[0m bernoulli_bandit \u001b[38;5;241m=\u001b[39m BernoulliBandit(n_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, probs\u001b[38;5;241m=\u001b[39mprobs)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Create one of each agent\u001b[39;00m\n\u001b[1;32m     64\u001b[0m agents \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     65\u001b[0m     EpsilonGreedyAgent(n_arms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m),\n\u001b[1;32m     66\u001b[0m     UCBAgent(n_arms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m),\n\u001b[0;32m---> 67\u001b[0m     ThompsonSamplingAgent(n_arms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     68\u001b[0m ]\n\u001b[1;32m     69\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpsilon-Greedy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUCB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThompson Sampling\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Run and plot\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ThompsonSamplingAgent.__init__() got an unexpected keyword argument 'n_arms'"
     ]
    }
   ],
   "source": [
    "\n",
    "class BernoulliBandit:\n",
    "    \"\"\"\n",
    "    A Bernoulli bandit with K actions. Each action yields a reward of 1 with probability theta_k\n",
    "    and 0 otherwise, where theta_k is unknown to the agent but fixed over time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions=10, probs=None):\n",
    "        \"\"\"\n",
    "        Initializes the Bernoulli bandit.\n",
    "\n",
    "        Args:\n",
    "            n_actions (int): The number of available actions (arms).\n",
    "            probs (list or np.array): Optional array of probabilities for each action.\n",
    "        \"\"\"\n",
    "        if probs is not None:\n",
    "            # Convert probs to numpy array if it's a list\n",
    "            probs = np.array(probs, dtype=float)\n",
    "            if len(probs) != n_actions:\n",
    "                raise ValueError(f\"Number of probabilities ({len(probs)}) must match number of actions ({n_actions})\")\n",
    "            if not np.all((probs >= 0) & (probs <= 1)):\n",
    "                raise ValueError(\"All probabilities must be between 0 and 1\")\n",
    "            self._probs = probs\n",
    "        else:\n",
    "            self._probs = np.random.random(n_actions)\n",
    "            \n",
    "        self._initial_probs = np.copy(self._probs)\n",
    "        self.action_count = n_actions\n",
    "        self.means = self._probs  # For compatibility with existing code\n",
    "        \n",
    "    def pull_arm(self, action):\n",
    "        \"\"\"\n",
    "        Simulates pulling a lever (taking an action) and returns a reward.\n",
    "\n",
    "        Args:\n",
    "            action (int): The index of the action to take.\n",
    "\n",
    "        Returns:\n",
    "            float: 1.0 if a random number is less than the action's probability, 0.0 otherwise.\n",
    "        \"\"\"\n",
    "        if not (0 <= action < self.action_count):\n",
    "            raise ValueError(f\"Action {action} is out of bounds. Must be between 0 and {self.action_count - 1}\")\n",
    "\n",
    "        return float(np.random.random() < self._probs[action])\n",
    "\n",
    "    def optimal_reward(self):\n",
    "        \"\"\"\n",
    "        Returns the expected reward of the optimal action. Used for regret calculation.\n",
    "\n",
    "        Returns:\n",
    "            float: The maximum probability among all actions.\n",
    "        \"\"\"\n",
    "        return float(np.max(self._probs))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the bandit to its initial state (initial probabilities).\"\"\"\n",
    "        self._probs = np.copy(self._initial_probs)\n",
    "\n",
    "# Test with Bernoulli bandit\n",
    "T = 50\n",
    "probs = [0.3, 0.5, 0.7, 0.4, 0.6]\n",
    "bernoulli_bandit = BernoulliBandit(n_actions=5, probs=probs)\n",
    "\n",
    "# Create one of each agent\n",
    "agents = [\n",
    "    EpsilonGreedyAgent(n_arms=5, epsilon=0.1),\n",
    "    UCBAgent(n_arms=5),\n",
    "    ThompsonSamplingAgent(n_arms=5)\n",
    "]\n",
    "labels = ['Epsilon-Greedy', 'UCB', 'Thompson Sampling']\n",
    "\n",
    "# Run and plot\n",
    "regret = run_agents_on_bandit(bernoulli_bandit, agents, T)\n",
    "plot_regret_comparison(regret, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d252f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b72101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f20a555f25738f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:47:54.524050Z",
     "start_time": "2025-04-24T14:47:54.517525Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_agents_on_bandit(bandit, agents, T):\n",
    "    n_agents = len(agents)\n",
    "    rewards = np.zeros((n_agents, T))\n",
    "    optimal_mean = np.max(bandit.means)\n",
    "\n",
    "    for t in range(T):\n",
    "        for j, agent in enumerate(agents):\n",
    "            arm = agent.select_arm()\n",
    "            reward = bandit.pull_arm(arm)\n",
    "            agent.update(arm, reward)\n",
    "            rewards[j, t] = reward\n",
    "\n",
    "    cumulative_rewards = np.cumsum(rewards, axis=1)\n",
    "    regret = (np.arange(1, T + 1) * optimal_mean) - cumulative_rewards\n",
    "    return regret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317f471be27ee6ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:48:42.640019Z",
     "start_time": "2025-04-24T14:48:42.635196Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_regret_comparison(regret, labels):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.plot(regret[i], label=label)\n",
    "    plt.title('Cumulative Regret Comparison')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Regret')\n",
    "    plt.legend()\n",
    "    # plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "484d2f8c5cb2e36b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:48:44.651356Z",
     "start_time": "2025-04-24T14:48:44.348963Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GaussianBandit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m      3\u001b[0m true_means \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m1.2\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.8\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m bandit \u001b[38;5;241m=\u001b[39m GaussianBandit(true_means, std_dev\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create one of each agent\u001b[39;00m\n\u001b[1;32m      7\u001b[0m agents \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     EpsilonGreedyAgent(n_arms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m),\n\u001b[1;32m      9\u001b[0m     UCBAgent(n_arms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m     10\u001b[0m     ThompsonSamplingAgent(n_arms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     11\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GaussianBandit' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "T = 10000\n",
    "true_means = [1.0, 1.5, 1.2, 0.8, 1.8]\n",
    "bandit = GaussianBandit(true_means, std_dev=0.5)\n",
    "\n",
    "# Create one of each agent\n",
    "agents = [\n",
    "    EpsilonGreedyAgent(n_arms=5, epsilon=0.1),\n",
    "    UCBAgent(n_arms=5),\n",
    "    ThompsonSamplingAgent(n_arms=5)\n",
    "]\n",
    "labels = ['Epsilon-Greedy', 'UCB', 'Thompson Sampling']\n",
    "\n",
    "# Run and plot\n",
    "regret = run_agents_on_bandit(bandit, agents, T)\n",
    "plot_regret_comparison(regret, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698ee1e74c68a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
